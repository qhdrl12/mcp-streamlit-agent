from typing import Any, Dict, List, Callable, Optional
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import CompiledStateGraph
from .stream_types import MessageChunk, UpdateChunk, StreamResult

async def handle_message_stream(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig],
    node_names: List[str],
    callback: Optional[Callable[[StreamResult], Any]] = None
) -> StreamResult:
    """ë©”ì‹œì§€ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í•¸ë“¤ëŸ¬

    Args:
        graph: ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ì…ë ¥ ë°ì´í„°
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì²˜ë¦¬í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡
        callback: ì½œë°± í•¨ìˆ˜

    Returns:
        StreamResult: ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬ëœ ì²­í¬ ì •ë³´
    """
    final_result: Optional[MessageChunk] = None

    prev_node = ""

    async for chunk_msg, metadata in graph.astream(
        inputs, config, stream_mode="messages"
    ):
        print(f"chunk_msg: {chunk_msg}, metadata: {metadata}")
        curr_node = metadata["langgraph_node"]
        
        # node_names í•„í„°ë§
        if node_names and curr_node not in node_names:
            continue
            
        result: MessageChunk = {
            "node": curr_node,
            "content": chunk_msg.content,
            "response_metadata": chunk_msg.response_metadata,
            "metadata": metadata
        }

        if callback:
            callback_result = callback(result)
            if hasattr(callback_result, "__await__"):
                await callback_result
        else:
            # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
            if curr_node != prev_node:
                print("\n" + "=" * 50)
                print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                print("- " * 25)


        final_result = result

    return final_result 

async def handle_update_stream(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig],
    node_names: List[str],
    include_subgraphs: bool,
    callback: Optional[Callable[[StreamResult], Any]] = None
) -> StreamResult:
    """ì—…ë°ì´íŠ¸ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í•¸ë“¤ëŸ¬

    Args:
        graph: ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ì…ë ¥ ë°ì´í„°
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì²˜ë¦¬í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡
        include_subgraphs: ì„œë¸Œê·¸ë˜í”„ í¬í•¨ ì—¬ë¶€
        callback: ì½œë°± í•¨ìˆ˜

    Returns:
        StreamResult: ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬ëœ ì²­í¬ ì •ë³´
    """
    final_result: Optional[UpdateChunk] = None

    async for chunk in graph.astream(
        inputs, config, stream_mode="updates", subgraphs=include_subgraphs
    ):  
        print(f"  ì—…ë°ì´íŠ¸ ì²­í¬: {chunk}")
        # ì²­í¬ í˜•ì‹ ì²˜ë¦¬
        namespace: List[str] = []
        node_chunks: Dict[str, Any] = {}

        if isinstance(chunk, tuple) and len(chunk) == 2:
            namespace, node_chunks = chunk
        else:
            node_chunks = chunk

        
        if isinstance(node_chunks, dict):
            for node_name, node_chunk in node_chunks.items():
                if node_names and node_name not in node_names:
                    continue
                    
                result: UpdateChunk = {
                    "node": node_name,
                    "content": node_chunk,
                    "namespace": namespace
                }
                print(f"result: {result}")
                if callback:
                    callback_result = callback(result)
                    if hasattr(callback_result, "__await__"):
                        await callback_result

                final_result = result
        else:
            result: UpdateChunk = {
                "node": "raw_output",
                "content": node_chunks,
                "namespace": namespace if isinstance(chunk, tuple) else []
            }
            
            if callback:
                callback_result = callback(result)
                if hasattr(callback_result, "__await__"):
                    await callback_result

            final_result = result

    return final_result 

# ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ
if __name__ == "__main__":
    import asyncio
    from langchain_core.messages import HumanMessage
    from langgraph.prebuilt import create_react_agent
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_google_genai import ChatGoogleGenerativeAI
    from dotenv import load_dotenv
    
    load_dotenv()


    async def main():
        print("handle_update_stream ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
        
        # model = ChatOpenAI(
        #     model="gpt-4o-mini",
        #     temperature=0.1,
        #     max_tokens=4096
        # )
        # model = ChatAnthropic(
        #     model="claude-3-5-haiku-20241022",
        #     temperature=0.1,
        #     max_tokens=8192
        # )

        model = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0.1,
            max_tokens=8192
        )


        agent = create_react_agent(
            model,
            tools=[],
            prompt="ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
        )

        # í…ŒìŠ¤íŠ¸ìš© ì—…ë°ì´íŠ¸ ì²­í¬ ìƒì„±
        # Mock ê·¸ë˜í”„ ì„¤ì •
        # ì½œë°± í•¨ìˆ˜ ì •ì˜
        async def print_updates_callback(chunk):
            print("-[print_callback]-")
            print(f"ë…¸ë“œ: {chunk['node']}")
            print(f"ë‚´ìš©: {chunk['content']}")
            print(f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {chunk['namespace']}")
            print("-" * 40)
        
        async def print_messages_callback(chunk):
            print(chunk['content'], end="", flush=True)

        
        # handle_update_stream ì‹¤í–‰
        # final_result = await handle_update_stream(
        #     graph=agent,
        #     inputs={"messages": [HumanMessage(content="ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")]},
        #     config=None,
        #     node_names=[],  # ëª¨ë“  ë…¸ë“œ ì²˜ë¦¬
        #     include_subgraphs=True,
        #     callback=print_updates_callback
        # )
        final_result = await handle_message_stream(
            graph=agent,
            inputs={"messages": [HumanMessage(content="ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")]},
            config=None,
            node_names=[],  # ëª¨ë“  ë…¸ë“œ ì²˜ë¦¬
            callback=print_messages_callback
        )
        
        print("\nìµœì¢… ê²°ê³¼:")
        print(f"ë…¸ë“œ: {final_result['node']}")
        print(f"ë‚´ìš©: {final_result['content']}")
        print(final_result)
        # print(f"ë‚´ìš©: {final_result['content']['messages'][0].content}")
    
    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(main())



