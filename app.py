from typing import Any, Callable, Dict, List
import uuid
import streamlit as st
import asyncio
import nest_asyncio
import json
from langchain_core.callbacks import AsyncCallbackHandler

# nest_asyncio ì ìš©: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œ ì¤‘ì²© í˜¸ì¶œ í—ˆìš©
nest_asyncio.apply()

# ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì¬ì‚¬ìš© (í•œë²ˆ ìƒì„±í•œ í›„ ê³„ì† ì‚¬ìš©)
if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)

from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.state import CompiledStateGraph
from langchain_core.runnables import RunnableConfig

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“±ì˜ ì„¤ì •ì„ ê°€ì ¸ì˜´)
load_dotenv(override=True)


async def astream_graph(
    graph: CompiledStateGraph,
    inputs: Dict[str, Any],
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable[[Dict[str, str]], None] = None,
):
    """
    Asynchronously streams the execution results of a LangGraph.

    Parameters:
    - graph (CompiledStateGraph): The compiled LangGraph to be executed.
    - inputs (dict): The input data dictionary to be passed to the graph.
    - config (RunnableConfig): Execution configuration.
    - node_names (List[str], optional): List of node names to filter output (empty list means all nodes).
    - callback (Callable[[Dict[str, str]], None], optional): A callback function for processing each chunk.
      The callback receives a dictionary with keys "node" (str) and "content" (str).

    Returns:
    - None: This function prints the streaming output but does not return any value.
    """    
    # final_result = {}

    prev_node = ""

    async for chunk_msg, metadata in graph.astream(
        inputs, config, stream_mode="messages"
    ):
        curr_node = metadata["langgraph_node"]
        # final_result = {"node": curr_node, "content": chunk_msg, "metadata": metadata}
        # Process only the specified nodes if node_names is not empty
        if not node_names or curr_node in node_names:
            if callback:
                callback({"node": curr_node, "content": chunk_msg})
            else:
                if curr_node != prev_node:
                    print("\n" + "=" * 50)
                    print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    print("- " * 25)
                                        
            prev_node = curr_node            

    # return final_result
# í˜ì´ì§€ ì„¤ì •: ì œëª©, ì•„ì´ì½˜, ë ˆì´ì•„ì›ƒ êµ¬ì„±
st.set_page_config(page_title="Agent with MCP Tools", page_icon="ğŸ§ ", layout="wide")

# ì‚¬ì´ë“œë°” ìµœìƒë‹¨ì— ì €ì ì •ë³´ ì¶”ê°€ (ë‹¤ë¥¸ ì‚¬ì´ë“œë°” ìš”ì†Œë³´ë‹¤ ë¨¼ì € ë°°ì¹˜)
st.sidebar.markdown("### âœï¸ Made by [í…Œë””ë…¸íŠ¸](https://youtube.com/c/teddynote) ğŸš€")
st.sidebar.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

# ê¸°ì¡´ í˜ì´ì§€ íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("ğŸ¤– Agent with MCP Tools")
st.markdown("âœ¨ MCP ë„êµ¬ë¥¼ í™œìš©í•œ ReAct ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "session_initialized" not in st.session_state:
    st.session_state.session_initialized = False  # ì„¸ì…˜ ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸
    st.session_state.agent = None  # ReAct ì—ì´ì „íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    st.session_state.mcp_client = None  # MCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.selected_model = "claude"  # ê¸°ë³¸ ëª¨ë¸ë¡œ Claude ì„¤ì •

if "thread_id" not in st.session_state:
    st.session_state.thread_id = uuid.uuid4()


# --- í•¨ìˆ˜ ì •ì˜ ë¶€ë¶„ ---
def print_message():
    """
    ì±„íŒ… ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    for message in st.session_state.history:
        if message["role"] == "user":
            st.chat_message("user").markdown(message["content"])
        elif message["role"] == "assistant":
            st.chat_message("assistant").write(message["content"], unsafe_allow_html=True)
        elif message["role"] == "assistant_tool":
            with st.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                st.markdown(message["content"])

# ìƒˆë¡œìš´ ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ë“¤
class StreamlitCallbackHandler(AsyncCallbackHandler):
    """
    ê¸°ë³¸ Streamlit ì½œë°± í•¸ë“¤ëŸ¬
    """
    def __init__(self, text_placeholder, tool_placeholder):
        self.text_placeholder = text_placeholder
        self.tool_placeholder = tool_placeholder
        self.accumulated_text = []
        self.accumulated_tool = []

    async def streamlit_log_tokens(self, text: str):
        self.text_placeholder.markdown("".join(self.accumulated_text))

    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs) -> None:
        print(f"on_tool_start serialized: {serialized}, input_str: {input_str}, kwargs: {kwargs}")
        self.accumulated_tool.append("\n```json\n" + json.dumps(serialized, indent=2, ensure_ascii=False) + "\n```\n")
        self.accumulated_tool.append("\n```json\n" + input_str + "\n```\n")
        print(f"on_tool_start accumulated_tool: {self.accumulated_tool}")
        with self.tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
            st.markdown("".join(self.accumulated_tool))

    async def on_tool_end(self, output: str, **kwargs) -> None:
        print(f"on_tool_end output: {output}, kwargs: {kwargs}")
        
        # outputì—ì„œ content ì˜ì—­ì„ ì¶”ì¶œí•˜ì—¬ accumulated_toolì— ì¶”ê°€
        if hasattr(output, 'content'): # claude
            # í•œê¸€ ë¬¸ìê°€ ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¡œ í‘œì‹œë˜ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬
            try:
                # ë¬¸ìì—´ì´ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  íŒŒì‹±
                json_obj = json.loads(output.content)
                # ë‹¤ì‹œ í•œê¸€ì´ ì œëŒ€ë¡œ í‘œì‹œë˜ë„ë¡ JSONìœ¼ë¡œ ë³€í™˜
                formatted_content = json.dumps(json_obj, indent=2, ensure_ascii=False)
                self.accumulated_tool.append("\n```json\n" + formatted_content + "\n```\n")
            except (json.JSONDecodeError, TypeError):
                # JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë‚´ìš© ê·¸ëŒ€ë¡œ ì‚¬ìš©
                self.accumulated_tool.append("\n```json\n" + output.content + "\n```\n")
        else:
            self.accumulated_tool.append("\n```json\n" + output + "\n```\n")
        with self.tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
            st.markdown("".join(self.accumulated_tool))

    async def on_tool_error(self, error: Exception, **kwargs) -> None:
        with self.tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
            st.markdown("".join(self.accumulated_tool))

class ClaudeCallbackHandler(StreamlitCallbackHandler):
    """
    Claude/Anthropic ëª¨ë¸ìš© ì½œë°± í•¸ë“¤ëŸ¬
    """
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"[ClaudeCallbackHandler] token: {token} , kwargs : {kwargs}")
        if isinstance(token, list):
            content = token[0]
            if isinstance(content, dict) and "text" in content:
                self.accumulated_text.append(content["text"])
                await self.streamlit_log_tokens(token)

class GPTCallbackHandler(StreamlitCallbackHandler):
    """
    GPT/OpenAI ëª¨ë¸ìš© ì½œë°± í•¸ë“¤ëŸ¬
    """
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"[GPTCallbackHandler] token: {token} , kwargs : {kwargs}")
        self.accumulated_text.append(token)
        await self.streamlit_log_tokens(token)
        # self.text_placeholder.markdown("".join(self.accumulated_text))

class GeminiCallbackHandler(StreamlitCallbackHandler):
    """
    Gemini/Google ëª¨ë¸ìš© ì½œë°± í•¸ë“¤ëŸ¬
    """
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"[GeminiCallbackHandler] token: {token} , kwargs : {kwargs}")
        # if isinstance(token, dict) and "text" in token:
        #     text = token["text"]
        # else:
        #     text = token
        self.accumulated_text.append(token)
        await self.streamlit_log_tokens(token)
        # self.text_placeholder.markdown("".join(self.accumulated_text))

def get_model_callback_handler(model_type: str, text_placeholder, tool_placeholder) -> AsyncCallbackHandler:
    """
    ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì ì ˆí•œ ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    handlers = {
        "claude": ClaudeCallbackHandler,
        "openai": GPTCallbackHandler,
        "gemini": GeminiCallbackHandler
    }
    handler_class = handlers.get(model_type, StreamlitCallbackHandler)
    return handler_class(text_placeholder, tool_placeholder)

# ê¸°ì¡´ process_query í•¨ìˆ˜ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ìƒˆë¡œìš´ ë²„ì „ ì¶”ê°€
async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=60):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        if st.session_state.agent:
            # ìƒˆë¡œìš´ ì½œë°± í•¸ë“¤ëŸ¬ ì‚¬ìš©
            callback_handler = get_model_callback_handler(
                st.session_state.selected_model,
                text_placeholder,
                tool_placeholder
            )
            
            try:
                response = await asyncio.wait_for(
                    astream_graph(
                        st.session_state.agent,
                        {"messages": [HumanMessage(content=query)]},
                        config=RunnableConfig(
                            callbacks=[callback_handler],
                            recursion_limit=100,
                            thread_id=st.session_state.thread_id
                        ),
                    ),
                    timeout=timeout_seconds,
                )
                
            except asyncio.TimeoutError:
                error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                return {"error": error_msg}, error_msg, ""
            
            final_text = "".join(callback_handler.accumulated_text)
            final_tool = "".join(callback_handler.accumulated_tool)

            return response, final_text, final_tool

        else:
            return (
                {"error": "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
    except Exception as e:
        import traceback
        error_msg = f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""


async def initialize_session(mcp_config=None):
    """
    MCP ì„¸ì…˜ê³¼ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        mcp_config: MCP ë„êµ¬ ì„¤ì • ì •ë³´(JSON). Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©

    ë°˜í™˜ê°’:
        bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    try:
        with st.spinner("ğŸ”„ MCP ì„œë²„ì— ì—°ê²° ì¤‘..."):
            print("inner MCP ì„œë²„ì— ì—°ê²° ì¤‘...")
            # ì´ì „ MCP í´ë¼ì´ì–¸íŠ¸ê°€ ì¡´ì¬í•˜ë©´ ë¨¼ì € ì •ë¦¬
            if st.session_state.mcp_client:
                try:
                    await st.session_state.mcp_client.__aexit__(None, None, None)
                except:
                    pass

            client = MultiServerMCPClient(mcp_config)
            await client.__aenter__()
            tools = client.get_tools()
            st.session_state.tool_count = len(tools)
            st.session_state.mcp_client = client

            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ LLM ì´ˆê¸°í™”
            selected_model = st.session_state.selected_model
            
            prompt = "Use your tools to answer the question. Answer in Korean."
            if selected_model == "claude":
                model = ChatAnthropic(
                    model="claude-3-5-haiku-20241022", 
                    temperature=0.1, 
                    max_tokens=8192
                )
            elif selected_model == "openai":
                model = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0.1,
                    max_tokens=4096
                )
            elif selected_model == "gemini":
                model = ChatGoogleGenerativeAI(
                    model="gemini-2.0-flash",
                    temperature=0.1,
                    max_tokens=8192
                )
            else:
                # ê¸°ë³¸ê°’ì€ Claudeë¡œ ì„¤ì •
                model = ChatAnthropic(
                    model="claude-3-5-haiku-20241022", 
                    temperature=0.1, 
                    max_tokens=8192
                )
            agent = create_react_agent(
                model,
                tools,
                checkpointer=MemorySaver(),
                prompt=prompt,
            )
            st.session_state.agent = agent
            st.session_state.session_initialized = True
            return True
    except Exception as e:
        st.error(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        st.error(traceback.format_exc())
        return False


# --- ì‚¬ì´ë“œë°” UI: MCP ë„êµ¬ ì¶”ê°€ ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€ê²½ ---
with st.sidebar.expander("MCP ë„êµ¬ ì¶”ê°€", expanded=False):
    default_config = """{
  "weather": {
    "command": "python",
    "args": ["./adapters/weather_server.py"],
    "transport": "stdio"
  }
}"""
    # pending configê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ mcp_config_text ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
    if "pending_mcp_config" not in st.session_state:
        # configs/default.json íŒŒì¼ì—ì„œ ê°’ì„ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
        with open("configs/default.json", "r") as f:
            default_json = json.load(f)
            
        # default.jsonì˜ mcpServers ë‚´ìš©ì„ pending_mcp_configì— í• ë‹¹
        if "mcpServers" in default_json:
            st.session_state.pending_mcp_config = default_json["mcpServers"]
        else:
            st.session_state.pending_mcp_config = default_json
            

    # ê°œë³„ ë„êµ¬ ì¶”ê°€ë¥¼ ìœ„í•œ UI
    st.subheader("ê°œë³„ ë„êµ¬ ì¶”ê°€")
    st.markdown(
        """
    **í•˜ë‚˜ì˜ ë„êµ¬**ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”:
    
    ```json
    {
      "ë„êµ¬ì´ë¦„": {
        "command": "ì‹¤í–‰ ëª…ë ¹ì–´",
        "args": ["ì¸ì1", "ì¸ì2", ...],
        "transport": "stdio"
      }
    }
    ```    
    âš ï¸ **ì¤‘ìš”**: JSONì„ ë°˜ë“œì‹œ ì¤‘ê´„í˜¸(`{}`)ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
    """
    )

    # ë³´ë‹¤ ëª…í™•í•œ ì˜ˆì‹œ ì œê³µ
    example_json = {
        "github": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@smithery-ai/github",
                "--config",
                '{"githubPersonalAccessToken":"your_token_here"}',
            ],
            "transport": "stdio",
        }
    }

    default_text = json.dumps(example_json, indent=2, ensure_ascii=False)

    new_tool_json = st.text_area(
        "ë„êµ¬ JSON",
        default_text,
        height=250,
    )

    # ì¶”ê°€í•˜ê¸° ë²„íŠ¼
    if st.button(
        "ë„êµ¬ ì¶”ê°€",
        type="primary",
        key="add_tool_button",
        use_container_width=True,
    ):
        try:
            # ì…ë ¥ê°’ ê²€ì¦
            if not new_tool_json.strip().startswith(
                "{"
            ) or not new_tool_json.strip().endswith("}"):
                st.error("JSONì€ ì¤‘ê´„í˜¸({})ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ì•¼ í•©ë‹ˆë‹¤.")
                st.markdown('ì˜¬ë°”ë¥¸ í˜•ì‹: `{ "ë„êµ¬ì´ë¦„": { ... } }`')
            else:
                # JSON íŒŒì‹±
                parsed_tool = json.loads(new_tool_json)

                # mcpServers í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
                if "mcpServers" in parsed_tool:
                    # mcpServers ì•ˆì˜ ë‚´ìš©ì„ ìµœìƒìœ„ë¡œ ì´ë™
                    parsed_tool = parsed_tool["mcpServers"]
                    st.info("'mcpServers' í˜•ì‹ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

                # ì…ë ¥ëœ ë„êµ¬ ìˆ˜ í™•ì¸
                if len(parsed_tool) == 0:
                    st.error("ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ë„êµ¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # ëª¨ë“  ë„êµ¬ì— ëŒ€í•´ ì²˜ë¦¬
                    success_tools = []
                    for tool_name, tool_config in parsed_tool.items():
                        # URL í•„ë“œ í™•ì¸ ë° transport ì„¤ì •
                        if "url" in tool_config:
                            # URLì´ ìˆëŠ” ê²½ìš° transportë¥¼ "sse"ë¡œ ì„¤ì •
                            tool_config["transport"] = "sse"
                            st.info(
                                f"'{tool_name}' ë„êµ¬ì— URLì´ ê°ì§€ë˜ì–´ transportë¥¼ 'sse'ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
                            )
                        elif "transport" not in tool_config:
                            # URLì´ ì—†ê³  transportë„ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ "stdio" ì„¤ì •
                            tool_config["transport"] = "stdio"

                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        if "command" not in tool_config and "url" not in tool_config:
                            st.error(
                                f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'command' ë˜ëŠ” 'url' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
                            )
                        elif "command" in tool_config and "args" not in tool_config:
                            st.error(
                                f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'args' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
                            )
                        elif "command" in tool_config and not isinstance(
                            tool_config["args"], list
                        ):
                            st.error(
                                f"'{tool_name}' ë„êµ¬ì˜ 'args' í•„ë“œëŠ” ë°˜ë“œì‹œ ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                            )
                        else:
                            # pending_mcp_configì— ë„êµ¬ ì¶”ê°€
                            st.session_state.pending_mcp_config[tool_name] = tool_config
                            success_tools.append(tool_name)

                    # ì„±ê³µ ë©”ì‹œì§€
                    if success_tools:
                        if len(success_tools) == 1:
                            st.success(
                                f"{success_tools[0]} ë„êµ¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                            )
                        else:
                            tool_names = ", ".join(success_tools)
                            st.success(
                                f"ì´ {len(success_tools)}ê°œ ë„êµ¬({tool_names})ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                            )
        except json.JSONDecodeError as e:
            st.error(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
            st.markdown(
                f"""
            **ìˆ˜ì • ë°©ë²•**:
            1. JSON í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.
            2. ëª¨ë“  í‚¤ëŠ” í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
            3. ë¬¸ìì—´ ê°’ë„ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
            4. ë¬¸ìì—´ ë‚´ì—ì„œ í°ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•  ê²½ìš° ì´ìŠ¤ì¼€ì´í”„(\\")í•´ì•¼ í•©ë‹ˆë‹¤.
            """
            )
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # êµ¬ë¶„ì„  ì¶”ê°€
    st.divider()

    # í˜„ì¬ ì„¤ì •ëœ ë„êµ¬ ì„¤ì • í‘œì‹œ (ì½ê¸° ì „ìš©)
    st.subheader("í˜„ì¬ ë„êµ¬ ì„¤ì • (ì½ê¸° ì „ìš©)")
    st.code(
        json.dumps(st.session_state.pending_mcp_config, indent=2, ensure_ascii=False)
    )

# --- ë“±ë¡ëœ ë„êµ¬ ëª©ë¡ í‘œì‹œ ë° ì‚­ì œ ë²„íŠ¼ ì¶”ê°€ ---
with st.sidebar.expander("ë“±ë¡ëœ ë„êµ¬ ëª©ë¡", expanded=True):
    try:
        pending_config = st.session_state.pending_mcp_config
    except Exception as e:
        st.error("ìœ íš¨í•œ MCP ë„êµ¬ ì„¤ì •ì´ ì•„ë‹™ë‹ˆë‹¤.")
    else:
        # pending configì˜ í‚¤(ë„êµ¬ ì´ë¦„) ëª©ë¡ì„ ìˆœíšŒí•˜ë©° í‘œì‹œ
        for tool_name in list(pending_config.keys()):
            col1, col2 = st.columns([8, 2])
            col1.markdown(f"- **{tool_name}**")
            if col2.button("ì‚­ì œ", key=f"delete_{tool_name}"):
                # pending configì—ì„œ í•´ë‹¹ ë„êµ¬ ì‚­ì œ (ì¦‰ì‹œ ì ìš©ë˜ì§€ëŠ” ì•ŠìŒ)
                del st.session_state.pending_mcp_config[tool_name]
                st.success(
                    f"{tool_name} ë„êµ¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                )

with st.sidebar:
    # ëª¨ë¸ ì„ íƒ UI ì¶”ê°€
    st.subheader("ğŸš€ ëª¨ë¸ ì„ íƒ")
    model_options = {
        "claude": "Claude 3.5 Haiku (Anthropic)",
        "openai": "GPT-4o (OpenAI)",
        "gemini": "Gemini 1.5 Pro (Google)"
    }
    
    selected_model = st.selectbox(
        "LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
        list(model_options.keys()),
        format_func=lambda x: model_options[x],
        index=list(model_options.keys()).index(st.session_state.selected_model)
    )
    
    # ì„ íƒëœ ëª¨ë¸ì´ ë³€ê²½ë˜ë©´ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì¬ì´ˆê¸°í™”
    if selected_model != st.session_state.selected_model:
        st.session_state.selected_model = selected_model
        st.session_state.session_initialized = False
        st.session_state.agent = None
        st.warning(f"ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ë¥¼ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        try:
            success = st.session_state.event_loop.run_until_complete(
                initialize_session(st.session_state.pending_mcp_config)
            )
            
            if success:
                st.success("âœ… ìƒˆë¡œìš´ ëª¨ë¸ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ ìƒˆë¡œìš´ ëª¨ë¸ ì ìš©ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ ëª¨ë¸ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
        st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
    
    st.divider()
    
    # ì ìš©í•˜ê¸° ë²„íŠ¼: pending configë¥¼ ì‹¤ì œ ì„¤ì •ì— ë°˜ì˜í•˜ê³  ì„¸ì…˜ ì¬ì´ˆê¸°í™”
    if st.button(
        "ë„êµ¬ì„¤ì • ì ìš©í•˜ê¸°",
        key="apply_button",
        type="primary",
        use_container_width=True,
    ):
        # ì ìš© ì¤‘ ë©”ì‹œì§€ í‘œì‹œ
        apply_status = st.empty()
        with apply_status.container():
            st.warning("ğŸ”„ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
            progress_bar = st.progress(0)

            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            progress_bar.progress(30)
            
            # ì„¸ì…˜ ì´ˆê¸°í™” ì¤€ë¹„
            st.session_state.session_initialized = False
            st.session_state.agent = None
            st.session_state.mcp_client = None

            print(f"st.session_state.pending_mcp_config: {st.session_state.pending_mcp_config}")

            # ì´ˆê¸°í™” ì‹¤í–‰
            try:
                success = st.session_state.event_loop.run_until_complete(
                    initialize_session(st.session_state.pending_mcp_config)
                )
                
                # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                progress_bar.progress(100)

                if success:
                    st.success("âœ… ìƒˆë¡œìš´ MCP ë„êµ¬ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ìƒˆë¡œìš´ MCP ë„êµ¬ ì„¤ì • ì ìš©ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.")
            except Exception as e:
                progress_bar.progress(100)
                st.error(f"âŒ ë„êµ¬ ì„¤ì • ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()


# --- ê¸°ë³¸ ì„¸ì…˜ ì´ˆê¸°í™” (ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°) ---
if not st.session_state.session_initialized:
    st.info("ğŸ”„ MCP ì„œë²„ì™€ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    success = st.session_state.event_loop.run_until_complete(
        initialize_session(st.session_state.pending_mcp_config)
    )
    if success:
        st.success(
            f"âœ… ì´ˆê¸°í™” ì™„ë£Œ! {st.session_state.tool_count}ê°œì˜ ë„êµ¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
    else:
        st.error("âŒ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ ì£¼ì„¸ìš”.")


# --- ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ---
print_message()

# --- ì‚¬ìš©ì ì…ë ¥ ë° ì²˜ë¦¬ ---
user_query = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_query:
    if st.session_state.session_initialized:
        st.chat_message("user").markdown(user_query)
        with st.chat_message("assistant"):
            tool_placeholder = st.empty()
            text_placeholder = st.empty()
            resp, final_text, final_tool = (
                st.session_state.event_loop.run_until_complete(
                    process_query(user_query, text_placeholder, tool_placeholder)
                )
            )

        print(f"chat resp: {resp}\nfinal_text: {final_text}\nfinal_tool: {final_tool}")
        if resp and "error" in resp:
            st.error(resp["error"])
        else:
            # ëª¨ë¸ë³„ íƒœê·¸ ìŠ¤íƒ€ì¼ ì •ì˜
            model_tags = {
                "claude": {"name": "Claude", "color": "orange"},
                "openai": {"name": "GPT", "color": "black"},
                "gemini": {"name": "Gemini", "color": "blue"}
            }
            
            current_model = st.session_state.selected_model
            model_info = model_tags.get(current_model, {"name": "AI", "color": "gray"})
            
            # HTML ìŠ¤íƒ€ì¼ì˜ íƒœê·¸ ìƒì„±
            model_tag = f'<span style="background-color: {model_info["color"]}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; margin-right: 8px;">{model_info["name"]}</span>'
            
            # íƒœê·¸ë¥¼ ë‹µë³€ ì•ì— ì¶”ê°€
            final_text_with_tag = f"{model_tag}{final_text}"

            st.session_state.history.append({"role": "user", "content": user_query})
            st.session_state.history.append(
                {"role": "assistant", "content": final_text_with_tag}
            )
            if final_tool.strip():
                st.session_state.history.append(
                    {"role": "assistant_tool", "content": final_tool}
                )
            st.rerun()
    else:
        st.warning("â³ ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# --- ì‚¬ì´ë“œë°”: ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ ---
with st.sidebar:
    st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
    st.write(f"ğŸ› ï¸ MCP ë„êµ¬ ìˆ˜: {st.session_state.get('tool_count', 'ì´ˆê¸°í™” ì¤‘...')}")
    
    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ í‘œì‹œ
    model_display_names = {
        "claude": "Claude 3.5 Haiku (Anthropic)",
        "openai": "GPT-4o (OpenAI)",
        "gemini": "Gemini 1.5 Pro (Google)"
    }
    st.write(f"ğŸš€ í˜„ì¬ ëª¨ë¸: {model_display_names.get(st.session_state.selected_model, 'ì•Œ ìˆ˜ ì—†ìŒ')}")

    # êµ¬ë¶„ì„  ì¶”ê°€ (ì‹œê°ì  ë¶„ë¦¬)
    st.divider()

    # ì‚¬ì´ë“œë°” ìµœí•˜ë‹¨ì— ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True, type="primary"):
        # thread_id ì´ˆê¸°í™”
        st.session_state.thread_id = uuid.uuid4()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        st.session_state.history = []

        # ì•Œë¦¼ ë©”ì‹œì§€
        st.success("âœ… ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()

        